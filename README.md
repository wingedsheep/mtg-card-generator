# MTG Card Generator

Generate complete Magic: The Gathering card sets using AI, including card mechanics, flavor text, and artwork. The generator creates thematically cohesive sets with synergistic mechanics based on randomly generated themes or your own custom themes.

## Features

- **Theme Options**: Either generate unique, cohesive themes automatically or provide your own complete theme override.
- **Card Generation**: Generates complete cards including:
  - Card names and mana costs
  - Card types and abilities
  - Power/toughness for creatures
  - Flavor text
  - Rarity distribution
  - Color balance
- **Art Generation**: Creates unique artwork for each card using AI.
  - **Cloud-based**: Supports Replicate API with models like FLUX and Imagen.
  - **Local generation**: Supports local image generation via Hugging Face Diffusers library (recommended for cost savings).
- **Language Model Integration**:
  - Uses OpenRouter API for card text, theme, and JSON conversion (default).
  - **Local option**: Supports local language model interaction via Ollama (requires local Ollama setup and downloaded models).
  - **Docker option**: Easy Docker Compose setup for local Ollama deployment.
- **Card Rendering**: Renders cards in the official MTG card frame style.
- **Format Support**: Outputs cards in both JSON and PNG formats.
- **Tabletop Simulator Support**: Convert card images into properly formatted deck sheets for Tabletop Simulator.
- **Booster Draft Generator**: Create draft boosters with the correct card distribution for play in Tabletop Simulator.

## Generated Example Cards

Here are some examples of cards generated by the system:

![Example Card 1](example-cards/example1.png)
![Example Card 2](example-cards/example2.png)
![Example Card 3](example-cards/example3.png)

## Prerequisites

- Python 3.8+
- Node.js (for running the card renderer)
- A modern web browser
- Docker and Docker Compose (optional, for local Ollama setup)

### API Keys (for cloud-based generation)

- **OpenRouter API key** (for card generation)
  - Sign up at https://openrouter.ai/
  - Create an account and generate an API key
  - This is used for generating card mechanics, flavor text, and converting card formats
- **Replicate API key** (for cloud-based art generation)
  - Sign up at https://replicate.com/
  - Create an account and generate an API key from your account settings
  - This is used for generating unique artwork for each card using AI models

### Local Generation Options (no API costs)

- **Hugging Face Diffusers** (for local image generation)
  - No API key needed, but requires local environment with GPU recommended
  - Supports models like Stable Diffusion XL, Stable Diffusion 1.5, etc.
  - First-time setup downloads model files (~2-7GB depending on model)
- **Ollama** (for local language model usage)
  - No API key needed, but requires local Ollama installation and downloaded models
  - Supports models like Llama 3, Mistral, etc.
  - Ollama website: https://ollama.com/
  - **Easy Docker setup available** (see Docker Setup section below)

## Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/mtg-card-generator.git
cd mtg-card-generator
```

2. Create and activate a virtual environment:
```bash
# On Linux/Mac
python -m venv venv
source venv/bin/activate

# On Windows
python -m venv venv
venv\Scripts\activate
```

3. Install PyTorch with GPU support (recommended for local image generation):

**For NVIDIA GPUs (Windows/Linux):**
```bash
# CUDA 11.8 (most compatible)
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1 (newer, requires compatible drivers)
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

**For Apple Silicon Macs (M1/M2/M3):**
```bash
# Install with Metal Performance Shaders (MPS) support
pip3 install torch torchvision torchaudio
```

**For AMD GPUs or CPU-only:**
```bash
# CPU-only version (slower for image generation)
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

**Check your installation:**
```bash
python -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'MPS available: {torch.backends.mps.is_available()}' if hasattr(torch.backends, 'mps') else 'MPS not supported')"
```

4. Install Python dependencies:

**For local image generation (recommended):**
```bash
pip install -r requirements.txt
pip install accelerate
pip install git+https://github.com/huggingface/diffusers
```

**For cloud-only generation:**
```bash
pip install -r requirements.txt
```

5. Install Playwright browser:
```bash
playwright install
```

6. Configure your setup:
```bash
# Copy the example settings file
cp card-generator/settings.example.json card-generator/settings.json

# Edit settings.json with your preferences and API keys (if using cloud services)
```

## Docker Setup (Local Ollama with GPU Support)

For easy local language model setup using Docker:

### 1. Prerequisites for Docker Setup

- Docker and Docker Compose installed
- NVIDIA Docker runtime (for GPU support) - optional but recommended

**Install NVIDIA Docker (Linux/WSL2):**
```bash
# Add NVIDIA package repositories
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

# Install nvidia-docker2
sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

### 2. Configure Storage (Important!)

Ollama models can be very large (4-70GB+ each). The default configuration uses a host directory which is recommended:

**Default: Host Directory (Recommended):**
```bash
# Create a directory for Ollama data (done automatically on first run)
mkdir -p ./ollama-data

# This is already configured in docker-compose.yml as:
# - ./ollama-data:/root/.ollama
```

**Alternative Options:**

**Option A: Absolute Path (for specific location):**
```bash
# Create directory on a drive with lots of space
sudo mkdir -p /var/lib/ollama

# Edit docker-compose.yml and change to:
# - /var/lib/ollama:/root/.ollama
```

**Option B: Named Volume (if you prefer Docker-managed storage):**
```bash
# Edit docker-compose.yml and change to:
# - ollama_data:/root/.ollama
# (May be limited by Docker's storage space)
```

### 3. Start Ollama with Docker

```bash
# Start Ollama and optional web UI
docker-compose up -d

# Check if services are running
docker-compose ps

# Check available disk space for models
docker exec mtg-ollama df -h /root/.ollama
```

This will start:
- **Ollama server** on port `11434`
- **Ollama Web UI** on port `8080` (optional, for easier model management)

### 4. Download and Use Models

**Model Size Guidelines:**
- **Small models (1-3GB)**: `llama3.2:1b`, `phi3.5:3.8b`
- **Medium models (4-8GB)**: `llama3:8b`, `mistral:7b`
- **Large models (15-40GB)**: `llama3:70b`, `mixtral:8x7b`
- **Very large models (50GB+)**: `llama3:405b`

**Check available space first:**
```bash
# Check available disk space
docker exec mtg-ollama df -h /root/.ollama
```

**Option A: Using Docker exec (recommended):**
```bash
# Download Llama 3 model (4.7GB)
docker exec mtg-ollama ollama pull llama3:8b

# Download Mistral model (4.1GB) - good for JSON tasks
docker exec mtg-ollama ollama pull mistral:7b

# Download smaller models for faster inference and less storage
docker exec mtg-ollama ollama pull llama3.2:1b    # 1.3GB
docker exec mtg-ollama ollama pull phi3.5:3.8b    # 2.2GB

# For high-end systems with lots of storage
docker exec mtg-ollama ollama pull llama3:70b     # 40GB

# List downloaded models and their sizes
docker exec mtg-ollama ollama list
```

**Option B: Using the Web UI:**
1. Open http://localhost:8080 in your browser
2. Create an account (local only)
3. Go to Admin Panel â†’ Models
4. Download models directly from the interface

### 4. Configure for Docker Ollama

Edit your `card-generator/settings.json`:
```json
{
  "language_model": {
    "strategy": "ollama",
    "ollama": {
      "host": "http://localhost:11434",
      "models": {
        "default_main": "llama3",
        "default_json": "mistral",
        "art_prompt_generation": "llama3",
        "theme_generation": "llama3",
        "card_batch_generation": "llama3",
        "json_conversion_from_text": "mistral",
        "render_format_conversion": "mistral"
      }
    }
  }
}
```

### 5. GPU Support for Docker

If you have an NVIDIA GPU, uncomment the GPU section in `docker-compose.yml`:

```yaml
# Uncomment for NVIDIA GPU support
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

Then restart the services:
```bash
docker-compose down
docker-compose up -d
```

### 6. Managing Docker Services

```bash
# Stop services
docker-compose down

# View logs
docker-compose logs ollama
docker-compose logs ollama-webui

# Update to latest images
docker-compose pull
docker-compose up -d

# Remove everything (including downloaded models)
docker-compose down -v
```

## Native Ollama Setup (Alternative to Docker)

If you prefer to install Ollama natively:

### Linux/WSL2:
```bash
curl -fsSL https://ollama.com/install.sh | sh

# Download models
ollama pull llama3
ollama pull mistral
```

### macOS:
```bash
# Install via Homebrew
brew install ollama

# Or download from https://ollama.com/
# Then download models
ollama pull llama3
ollama pull mistral
```

### Windows:
1. Download Ollama from https://ollama.com/
2. Install the executable
3. Open PowerShell and run:
```powershell
ollama pull llama3
ollama pull mistral
```

## Configuration

### Local Image Generation Setup (Recommended)

For free local image generation using Diffusers:

1. **Edit settings.json:**
```json
{
  "image_generation": {
    "strategy": "diffusers",
    "diffusers": {
      "model_id": "stabilityai/stable-diffusion-xl-base-1.0",
      "device": "auto",
      "params": {
        "num_inference_steps": 30,
        "guidance_scale": 7.5
      },
      "dimensions": {
        "standard_width": 1024,
        "standard_height": 1024,
        "saga_width": 768,
        "saga_height": 1024
      }
    }
  }
}
```

2. **Hardware Requirements:**
   - **NVIDIA GPU (recommended)**: 8GB+ VRAM for best performance with SDXL
   - **Apple Silicon (M1/M2/M3)**: 16GB+ unified memory recommended
   - **CPU (slower)**: Will work but expect 2-5 minutes per image
   - **AMD GPU**: Limited support, CPU fallback recommended

3. **Model Options:**
   - `stabilityai/stable-diffusion-xl-base-1.0` - High quality, requires more VRAM
   - `runwayml/stable-diffusion-v1-5` - Faster, less VRAM needed
   - `stabilityai/stable-diffusion-2-1` - Good balance

### Cloud-Based Generation Setup

For cloud-based generation using APIs:

1. **Get API keys** from OpenRouter and Replicate (see Prerequisites)

2. **Edit settings.json:**
```json
{
  "api_keys": {
    "openrouter": "your_openrouter_api_key",
    "replicate": "your_replicate_api_key"
  },
  "image_generation": {
    "strategy": "replicate"
  },
  "language_model": {
    "strategy": "openrouter"
  }
}
```

### Mixed Setup

You can also mix local and cloud services:
```json
{
  "image_generation": {
    "strategy": "diffusers"  // Local image generation
  },
  "language_model": {
    "strategy": "ollama"  // Local language model via Docker
  }
}
```

## Usage

1. Start the card generation process:
```bash
cd card-generator
python main.py
```

**First run with local generation:** The system will download the selected AI model (this may take 5-15 minutes depending on your internet connection).

**With Docker Ollama:** Make sure Docker services are running first:
```bash
docker-compose up -d
# Wait for Ollama to start, then run main.py
```

This will:
- Generate a random set theme (or use your provided theme)
- Create cards with balanced colors and rarities
- Generate art for each card
- Convert cards to the proper rendering format
- Render cards as images

2. The generated content will be in the `output_sets/[timestamp]` directory:
- `mtg_set_output.json`: Raw card data
- `mtg_set_complete.json`: Complete set data with statistics
- `render_format/`: Cards formatted for rendering
- `card_images/`: Final rendered card images

3. To manually render individual cards:
```bash
cd card-rendering
# Open index.html in a web browser
```

## Advanced Configuration

The configuration system uses a strategy pattern that allows you to easily switch between different AI providers:

### Image Generation Strategies

**Diffusers (Local):**
```json
{
  "image_generation": {
    "strategy": "diffusers",
    "diffusers": {
      "model_id": "stabilityai/stable-diffusion-xl-base-1.0",
      "device": "auto",  // "auto", "cuda", "mps", "cpu"
      "params": {
        "num_inference_steps": 30,  // Higher = better quality, slower
        "guidance_scale": 7.5,      // How closely to follow prompt
        "negative_prompt": "blurry, low quality, distorted, deformed"
      }
    }
  }
}
```

**Replicate (Cloud):**
```json
{
  "image_generation": {
    "strategy": "replicate",
    "replicate": {
      "selected_model_type": "flux",  // "flux" or "imagen"
      "models": {
        "flux": {
          "id": "black-forest-labs/flux-1.1-pro",
          "params": { /* model-specific parameters */ }
        }
      }
    }
  }
}
```

### Language Model Strategies

**OpenRouter (Cloud):**
```json
{
  "language_model": {
    "strategy": "openrouter",
    "openrouter": {
      "models": {
        "default_main": "openai/gpt-4o-2024-11-20",
        "art_prompt_generation": "openai/gpt-4o-2024-11-20",
        "theme_generation": "openai/gpt-4o-2024-11-20"
      }
    }
  }
}
```

**Ollama (Local/Docker):**
```json
{
  "language_model": {
    "strategy": "ollama",
    "ollama": {
      "host": "http://localhost:11434",  // Default for Docker setup
      "models": {
        "default_main": "llama3",
        "default_json": "mistral",
        "theme_generation": "llama3",
        "art_prompt_generation": "llama3",
        "card_batch_generation": "llama3",
        "json_conversion_from_text": "mistral",
        "render_format_conversion": "mistral"
      },
      "params": {
        "temperature": 0.8
      },
      "json_params": {
        "temperature": 0.3,
        "format": "json"
      }
    }
  }
}
```

### Operational Settings

```json
{
  "operational_settings": {
    "inspiration_cards_count": 50,
    "batches_count": 20,
    "theme_prompt": "Epic fantasy with dragons and ancient magic",
    "complete_theme_override": null,  // Or provide full theme text
    "generate_basic_lands": true,
    "land_variations_per_type": 3,
    "rarities_per_batch": {
      "mythic": 1, "rare": 3, "uncommon": 4, "common": 5
    },
    "color_distribution_targets": {
      "W": 0.2, "U": 0.2, "B": 0.2, "R": 0.2, "G": 0.2
    }
  }
}
```

## Custom Theme Structure

When providing a `complete_theme_override`, your theme should include:

```
# Theme Title

## World Description
[Detailed description of the world/setting]

## Key Factions
[List and description of major factions/groups]

## Creature Types
[Common creature types in the set]

## Mechanical Themes
[Key gameplay mechanics and themes]

## Synergies
[How different card types and mechanics work together]

## Play Styles
[What play styles the set supports]
```

## Basic Land Generation

The system automatically generates variations of each basic land type:

- Plains, Island, Swamp, Mountain, and Forest
- Each type gets multiple artistic variations
- Land art is themed to match your set's aesthetic

You can configure:

- Whether to generate basic lands (`generate_basic_lands`)
- How many variations to create for each type (`land_variations_per_type`)

## Tabletop Simulator Integration

The project includes a TTS Deck Converter tool (`tts_deck_converter.py`) that arranges your generated card images into grid layouts compatible with Tabletop Simulator.

### TTS Converter Features

- Creates properly formatted card sheets for importing into Tabletop Simulator
- Automatically handles large sets by creating multiple sheets when needed
- Configurable grid dimensions, card sizes, and output quality
- Optional card sorting for easier organization

### Using the TTS Converter

```bash
cd card-generator
python tts_deck_converter.py
```

This will open a folder selector dialog. Choose a directory containing your card images, and the converter will create TTS-compatible deck sheets.

## Booster Draft Generator

The project includes a Booster Draft Generator that creates randomized booster packs from your generated sets for drafting in Tabletop Simulator.

### Using the Booster Generator

1. Launch the booster generator:
```bash
cd card-generator
python mtg_booster_generator.py
```

2. In the interface:
   - Select your MTG set folder
   - Set the number of boosters (1-100)
   - Click "Generate Boosters"

The generator creates:
- 15-card boosters with the correct rarity distribution (1 rare/mythic, 3 uncommons, 11 commons)
- Special boosters for each basic land type with all art variants
- All output saved in a `boosters` folder ready for use in Tabletop Simulator

## Model Recommendations

### For Language Models (Ollama)

**Fast and Efficient:**
- `llama3.2:1b` - Very fast, good for rapid prototyping
- `phi3.5` - Small but capable, good balance

**High Quality:**
- `llama3` - Excellent general purpose model (8B parameters)
- `mistral` - Great for JSON tasks and structured output
- `llama3:70b` - Highest quality but requires significant resources

**Specialized:**
- `codellama` - Better for technical/structured text
- `neural-chat` - Good for creative writing

### For Image Models (Diffusers)

**High Quality:**
- `stabilityai/stable-diffusion-xl-base-1.0` - Best quality, requires 8GB+ VRAM
- `stabilityai/stable-diffusion-2-1` - Good quality, more efficient

**Fast/Efficient:**
- `runwayml/stable-diffusion-v1-5` - Classic model, works on 4GB VRAM
- `stabilityai/stable-diffusion-2-1-base` - Good balance of speed/quality

## Performance Optimization

### Local Image Generation

**For better performance:**
- Use NVIDIA GPU with CUDA support
- Use Apple Silicon with MPS support
- Ensure adequate VRAM/memory (8GB+ recommended for SDXL)
- Use lower resolution for faster generation
- Reduce `num_inference_steps` for speed

**For lower-end hardware:**
- Use `runwayml/stable-diffusion-v1-5` instead of SDXL
- Set device to `"cpu"` if GPU issues occur
- Reduce image dimensions to 512x512

### Local Language Model Performance

**Docker/Ollama optimization:**
- Use GPU-enabled Docker for faster inference
- Choose appropriate model sizes for your hardware
- Use smaller models (`llama3.2:1b`, `phi3.5`) for faster response
- Enable CPU optimization with `OLLAMA_NUM_PARALLEL=4` environment variable

### Memory Management

The system automatically:
- Enables attention slicing for memory efficiency
- Uses CPU offload for low-VRAM systems
- Cleans up GPU memory between generations
- Uses float16 precision on GPU for memory savings

## Troubleshooting

### Common Issues

**Docker/Ollama Issues:**
- `Connection refused`: Ensure Docker services are running with `docker-compose ps`
- `Model not found`: Download models with `docker exec mtg-ollama ollama pull llama3:8b`
- `GPU not detected`: Uncomment GPU section in docker-compose.yml and restart
- `Port conflicts`: Change ports in docker-compose.yml if 11434 or 8080 are in use
- `No space left on device`: 
  - Check space: `docker exec mtg-ollama df -h /root/.ollama`
  - Use host directory mount instead of named volume
  - Clean up unused models: `docker exec mtg-ollama ollama rm model_name`
- `Out of memory`: Use smaller models like `llama3.2:1b` or increase Docker memory limits

**PyTorch/GPU Setup:**
- `CUDA not available`: Ensure you installed the CUDA version of PyTorch and have compatible drivers
- `MPS not available on Mac`: Update to macOS 12.3+ and ensure you have Apple Silicon
- `GPU memory issues`: Reduce image dimensions or use CPU fallback

**Diffusers/Local Generation:**
- `CUDA out of memory`: Reduce image dimensions or use a smaller model
- `Model download fails`: Check internet connection, try different model
- `Slow generation on CPU`: This is normal; consider using cloud generation for speed

**General:**
- `API key errors`: Verify your API keys in settings.json
- `Missing dependencies`: Run `pip install -r requirements.txt` again
- `Playwright issues`: Run `playwright install` to reinstall browsers

### Hardware Requirements

**Minimum (CPU only):**
- 8GB RAM
- 20GB free disk space
- Patience (2-5 minutes per image, 30+ seconds per text generation)

**Recommended (NVIDIA GPU):**
- NVIDIA GPU with 8GB+ VRAM
- 16GB RAM
- 20GB free disk space
- Fast generation (10-30 seconds per image, 5-15 seconds per text)

**Recommended (Apple Silicon):**
- M1/M2/M3 Mac with 16GB+ unified memory
- 20GB free disk space
- Good generation speed (30-60 seconds per image, 10-20 seconds per text)

**Docker Resource Requirements:**
- Additional 8-16GB RAM for medium models (16-32GB for large models)
- **Storage: 20-200GB disk space** for model storage (varies greatly by models chosen)
  - Small setup (1-2 small models): 20-30GB
  - Medium setup (2-3 medium models): 50-80GB  
  - Large setup (1-2 large models): 100-200GB+
- GPU passthrough for optimal performance

## Card Rendering Details

The card renderer supports:
- Standard MTG card frames
- Extended art frames
- Various card types (creatures, instants, sorceries, etc.)
- Multicolored cards
- Power/toughness boxes
- Set symbols
- Collector numbers
- Artist credits

## Acknowledgments

- Card rendering system based on [MTG Render](https://www.mtgrender.tk/) by Yoann 'Senryoku' Maret-Verdant
- Magic: The Gathering is a trademark of Wizards of the Coast LLC
- Card frame designs based on official MTG templates
- AI art generation powered by:
  - **Local**: Hugging Face Diffusers, Stability AI (Stable Diffusion), and other open-source models
  - **Cloud**: Replicate, Black Forest Labs (FLUX), Google (Imagen), and other hosted models
- Card generation powered by:
  - **Local**: Ollama with Llama, Mistral, and other open-source language models
  - **Cloud**: OpenRouter, OpenAI, and various hosted language models

Special thanks to Yoann 'Senryoku' Maret-Verdant for creating the original MTG card renderer ([GitHub](https://github.com/Senryoku)) which forms the foundation of our card rendering system.